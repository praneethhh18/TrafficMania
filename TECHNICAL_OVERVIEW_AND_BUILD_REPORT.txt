SMART TRAFFIC RL - TECHNICAL OVERVIEW AND BUILD REPORT
=====================================================

Purpose
-------
This document explains, in a structured and detailed way, how the project was built—from the basic prototype to the current advanced system. It covers skills and technologies used, how the system works end-to-end, design choices, error handling strategies, training methodology, and guidance for real-world transition.


1) Skills Demonstrated
----------------------
- Python engineering
  - Modular design, CLI tooling, logging, configuration management
  - Defensive programming and robust error handling
- Reinforcement Learning (RL)
  - Double Dueling DQN, target networks, Double Q-learning
  - Experience Replay and Prioritized Experience Replay (PER)
  - Epsilon-greedy exploration schedules; Huber loss; gradient clipping; LR scheduling
  - Reward shaping for safety and throughput tradeoffs
- Traffic simulation and control with SUMO/TraCI
  - Traffic light control logic, phase management, min-green safety
  - Route generation (randomTrips), collision reporting, teleport safeguards
  - Violation detection and ambulance priority logic using TraCI APIs
- Observability & tooling
  - CSV logging (step-level and episode-level)
  - TensorBoard integration and live plotting support
- Computer vision integration hooks
  - YOLOv8 vehicle detection scaffolding (for perception/observer), ambulance detection link (notify_ambulance)
- Systems thinking and safety
  - Safe switching (min-green + decision interval), cooldowns
  - Rule enforcement and auditing (red-light violations, optional logs)
  - Shadow-mode deployment and safety-gates outline for real-world transfer


2) Technologies Used and Why
----------------------------
- SUMO + TraCI
  - SUMO: Microscopic traffic simulation to model realistic intersections and flows
  - TraCI: Python API to control the simulation (phases, lights, vehicles) step-by-step
  - Why: Industry-standard for traffic control experiments; reproducible; detailed APIs
- PyTorch
  - Neural networks and optimization for DQN variants
  - Why: Flexible, well-supported deep learning framework with mature tooling
- RL Algorithms
  - Dueling DQN: Separates value and advantage streams for more stable learning
  - Double Q-learning: Reduces overestimation bias by decoupling action selection from evaluation
  - PER: Samples high-TD-error transitions more often; importance sampling corrects bias
- Observability stack
  - CSV logs: Universal portability and quick analysis
  - TensorBoard: Live dashboards for training metrics
- Optional Perception
  - YOLOv8: Vehicle detection; can drive ambulance notifications in real time


3) System Architecture (How It Works)
-------------------------------------
- Environment: sumo_env_advanced.py
  - Reset: Boots SUMO or SUMO-GUI via TraCI; identifies the signal controller; builds lane→green-phase maps
  - Observation vector per step:
    - [per-lane halting numbers, total_queue, total_wait, current_phase_index, time_since_last_switch]
  - Action space:
    - 0: Keep current phase
    - 1: Request safe switch (only allowed by min-green and decision-interval rules)
  - Reward function (per step):
    - reward = -(alpha * total_queue + beta * total_wait + gamma * switch_penalty)
              - (collision_penalty * collisions)
              + (arrival_reward * arrivals)
              - (violation_penalty * red_light_violations)
    - Optional escalation for repeat violators via violation_reoffender_factor
  - Safety and rules:
    - Min-green (must hold green for at least N steps before switch)
    - Decision interval (only consider switching every K steps)
    - Ambulance priority:
      - Auto-detect emergency vehicles by vType keywords (e.g., "ambul", "emerg")
      - Request priority if vehicle is slow/standing at red/yellow
      - Direct switch to a serving phase with a cooldown to avoid thrashing
    - Red-light violation detection:
      - Tracks vehicles leaving lanes that were red on the previous phase
      - Per-step penalties and optional logging of violating IDs

- Trainer: train_dqn_advanced.py
  - Double Dueling DQN with target network; Double Q-targets
  - Replay buffer or Prioritized Replay (PER)
  - Epsilon schedule (start/end/decay steps)
  - Huber loss; gradient clipping; optional LR scheduling
  - Logging:
    - Step-level CSV (reward, total_q, total_w, phase, epsilon, collisions, arrivals)
    - Episode-level CSV (episode reward, average metrics, switches, sums of collisions/arrivals)
    - Optional TensorBoard
  - Checkpoints and best moving-average save (rolling MA of episode returns)
  - Optional route reseeding with randomTrips for curriculum or variability

- Policy Runner: run_dqn_policy.py
  - Loads model and runs either headless or GUI mode
  - Mirrors decision interval and min-green guardrails
  - Prints aggregate metrics at the end


4) How the Key Technologies Work (Short Primers)
------------------------------------------------
- Dueling DQN
  - Network outputs a scalar value (V) and an advantage vector (A) over actions
  - Combines to Q-values: Q(s,a) = V(s) + A(s,a) - mean_a A(s,a)
  - Stabilizes and speeds up learning when many actions exist or value dominates

- Double Q-learning
  - Uses online network to choose argmax action, but target network to evaluate its value
  - Reduces overestimation bias common in vanilla DQN targets

- Prioritized Experience Replay (PER)
  - Each transition gets a priority proportional to |TD-error| + eps
  - Sampling probability ~ priority^alpha; importance weights correct the bias (annealed beta)
  - Improves sample efficiency—focuses learning where the agent is most wrong

- SUMO/TraCI
  - SUMO simulates vehicles, lanes, TLS phases, detectors; TraCI lets us step the simulation and get/set state
  - Key calls:
    - trafficlight.setPhaseDuration / setPhase
    - getRedYellowGreenState / getPhase
    - lane.getLastStepHaltingNumber / getWaitingTime
    - simulation.getArrivedNumber / getCollidingVehiclesNumber


5) Error Handling and Robustness
--------------------------------
- Environment and TraCI calls safety
  - Try/except around TraCI getters (collisions, arrivals, states) with compatible fallbacks
  - SUMO_HOME check with clear error if not set
  - Teleport enabled for stuck vehicles to prevent deadlock during training
- Model loading
  - Safe loading with map_location; graceful warning if missing init model
- Logging resilience
  - CSV opens in write/append mode with minimal dependencies
  - TensorBoard initialization guarded; if TB not available, training continues
- Training stability
  - Huber loss, gradient clipping, target updates (hard or soft), epsilon scheduling
  - Min-green and decision intervals prevent unrealistic oscillations
- Network/route warnings
  - Teleports due to lane-turn mapping flagged for future network refinement (connections, TLS phases)


6) Training Methodology
-----------------------
- Baseline to advanced
  - Start with safe switching, basic DQN, then move to Double Dueling DQN
  - Add safety shaping: collisions, arrivals, switching penalty, queue/wait weights
  - Enable violation detection and ambulance priority; test GUI and headless
- Hyperparameters and shaping
  - alpha (queue penalty), beta (wait penalty), gamma-r (switch), arrival_reward, collision_penalty, violation_penalty
  - Min-green and decision-interval tuned to reduce thrashing
- Replay and exploration
  - PER with alpha≈0.6, beta annealed to 1.0; epsilon decays slowly to stability during fine-tune
- Observability and checkpoints
  - Per-step and per-episode CSVs; TensorBoard (optional)
  - Rolling best model saved via moving average of rewards
- Curriculum and reseeding (optional)
  - Route reseeding with randomTrips for variability and generalization


7) Evolution: From Scratch to Advanced
--------------------------------------
- Phase 1: Core environment and basic DQN
  - Simple keep/switch action, min-green safety, queue/wait reward
- Phase 2: Advanced environment
  - Collision reporting, arrivals reward, robust TraCI calls, teleport, CSV logs, plots
- Phase 3: Better RL
  - Double Dueling DQN, target network, Huber loss, gradient clipping, epsilon schedule
- Phase 4: Safety and rule enforcement
  - Red-light violation detection/penalty; violation logging
- Phase 5: Ambulance priority
  - Lane→phase mapping, detect ambulances, priority switching with cooldown
- Phase 6: Observability and longer runs
  - TensorBoard logging, headless/GUI evaluation, moving-average checkpoints
- Phase 7: PER and fine-tuning
  - Prioritized Replay; higher arrival_reward for more positive totals; refined epsilon schedule
- Phase 8: Documentation and real-world path
  - README with how-to, final metrics, and guidance on safe sim2real deployment


8) Final Metrics (Key Runs)
---------------------------
- Final fine-tune model (arrival_reward=4.5): dqn_final_45.pt
  - Headless 600 steps: total reward ≈ -1937.42, avg_total_q ≈ 1.18, avg_total_w ≈ 8.36, collisions=0, arrivals=245
  - GUI 300 steps: total reward ≈ -1236.79, avg_total_q ≈ 1.28, avg_total_w ≈ 9.86, collisions=0, arrivals=108

Note: Totals include queue/wait penalties; they can remain negative even while queues, waits, and safety improve. Zero collisions and low average wait indicate strong real-world alignment.


9) How to Operate (In Short)
-----------------------------
- Train (example):
  - train_dqn_advanced.py with your shaping (alpha, beta, gamma-r, arrival_reward, penalties), PER on, and checkpoints enabled
- Evaluate:
  - run_dqn_policy.py with the same shaping and guardrails (min-green, decision interval); run headless or GUI
- Ambulance integration:
  - Either rely on auto-detect by vType keywords or call notify_ambulance(lane_id) from your perception service


10) Real-World Transition Checklist
-----------------------------------
- Keep min-green and intergreens enforced in hardware
- Add a safety gate that vets AI phase changes before sending to the controller
- Start in shadow mode, compare against fixed-time/actuated plans
- Integrate perception for ambulance detection; log rule violations for audits
- Calibrate sim with real counts; randomize demand/weather; retrain periodically
- Respect regulations; use controller protocols (e.g., NTCIP) or vendor SDKs; secure sign-offs


11) Files and Their Roles (Key)
-------------------------------
- sumo_env_advanced.py: SUMO environment, rewards, safety logic, ambulance, violations
- train_dqn_advanced.py: Dueling DQN + PER trainer, logging, checkpoints
- run_dqn_policy.py: Headless/GUI evaluation and metrics
- README.md: How-to, metrics, sim-to-real guidance
- (Optional) routes/, net/, traffic_config*.sumocfg: Network and routing assets


12) Lessons Learned
-------------------
- Balancing safety and throughput requires careful reward shaping and guardrails
- PER accelerates learning on tricky edge cases (e.g., high queues at peak)
- Min-green and decision-interval constraints make policies more realistic and prevent oscillations
- Ambulance priority needs both logic (phase switching) and hygiene (cooldowns, only at red/yellow)
- Observability (CSV + TB) is key to diagnosing regressions and improvements


End of Report
-------------
